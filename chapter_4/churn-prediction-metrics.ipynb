{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T13:37:42.236447Z","iopub.execute_input":"2021-12-16T13:37:42.236855Z","iopub.status.idle":"2021-12-16T13:37:42.274465Z","shell.execute_reply.started":"2021-12-16T13:37:42.236783Z","shell.execute_reply":"2021-12-16T13:37:42.273733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's load and Prepare our data","metadata":{}},{"cell_type":"code","source":"## import all relevant libraries\nimport seaborn as sn\nfrom matplotlib import pyplot as plt\n%matplotlib inline \n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:42.27616Z","iopub.execute_input":"2021-12-16T13:37:42.27642Z","iopub.status.idle":"2021-12-16T13:37:43.349081Z","shell.execute_reply.started":"2021-12-16T13:37:42.276384Z","shell.execute_reply":"2021-12-16T13:37:43.348167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets load our dataset\ndf = pd.read_csv('../input/telecom-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.349968Z","iopub.execute_input":"2021-12-16T13:37:43.350149Z","iopub.status.idle":"2021-12-16T13:37:43.442279Z","shell.execute_reply.started":"2021-12-16T13:37:43.350126Z","shell.execute_reply":"2021-12-16T13:37:43.44134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets normalize all the column names in our dataset\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.444306Z","iopub.execute_input":"2021-12-16T13:37:43.44457Z","iopub.status.idle":"2021-12-16T13:37:43.469323Z","shell.execute_reply.started":"2021-12-16T13:37:43.444541Z","shell.execute_reply":"2021-12-16T13:37:43.468464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets check for column types\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.470833Z","iopub.execute_input":"2021-12-16T13:37:43.471216Z","iopub.status.idle":"2021-12-16T13:37:43.487292Z","shell.execute_reply.started":"2021-12-16T13:37:43.47119Z","shell.execute_reply":"2021-12-16T13:37:43.48653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets normalize the values in the our dataset\n##lets select all columns with string values\nstring_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor col in string_columns:\n    df[col] = df[col].str.lower().str.replace(' ', '_')\n    \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.488262Z","iopub.execute_input":"2021-12-16T13:37:43.488671Z","iopub.status.idle":"2021-12-16T13:37:43.609786Z","shell.execute_reply.started":"2021-12-16T13:37:43.488647Z","shell.execute_reply":"2021-12-16T13:37:43.609015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets now change the column type of totalcharges to a numeric type\ndf['totalcharges'] = pd.to_numeric(df['totalcharges'], errors='coerce')\n##lets fill the emplty values with zeros\ndf['totalcharges'] = df['totalcharges'].fillna(0)\n\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.611002Z","iopub.execute_input":"2021-12-16T13:37:43.611211Z","iopub.status.idle":"2021-12-16T13:37:43.624916Z","shell.execute_reply.started":"2021-12-16T13:37:43.611183Z","shell.execute_reply":"2021-12-16T13:37:43.623864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets change the values of predicted values in numerical values\ndf.churn = (df.churn == 'yes').astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.62613Z","iopub.execute_input":"2021-12-16T13:37:43.626806Z","iopub.status.idle":"2021-12-16T13:37:43.643537Z","shell.execute_reply.started":"2021-12-16T13:37:43.626776Z","shell.execute_reply":"2021-12-16T13:37:43.642677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split our data in train, validation, and test sets. ","metadata":{}},{"cell_type":"code","source":"##lets import all relevarant libraries\nfrom sklearn.model_selection import train_test_split\n\n##lets alocate 20% of our data to the test set\ndf_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\n\n## lets allocate 33 percent to our val set and the rest to train set\ndf_train, df_val = train_test_split(df_train_full, test_size=0.33, random_state =11)\n\n##lets select our target data\ny_train = df_train.churn.values\ny_val = df_val.churn.values\n\n## lets delete the target data from our training data\ndel df_train['churn']\ndel df_val['churn']","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.64482Z","iopub.execute_input":"2021-12-16T13:37:43.645862Z","iopub.status.idle":"2021-12-16T13:37:43.872442Z","shell.execute_reply.started":"2021-12-16T13:37:43.645803Z","shell.execute_reply":"2021-12-16T13:37:43.87143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets create a list of columns with categorical data\ncategorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n               'phoneservice', 'multiplelines', 'internetservice',\n               'onlinesecurity', 'onlinebackup', 'deviceprotection',\n               'techsupport', 'streamingtv', 'streamingmovies',\n               'contract', 'paperlessbilling', 'paymentmethod' ]\n\n## lets create a list of columns with numerical data\nnumerical = ['tenure', 'monthlycharges',  'totalcharges']","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.874822Z","iopub.execute_input":"2021-12-16T13:37:43.875549Z","iopub.status.idle":"2021-12-16T13:37:43.882638Z","shell.execute_reply.started":"2021-12-16T13:37:43.875515Z","shell.execute_reply":"2021-12-16T13:37:43.881991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check for unique values in each columns \ndf_train_full[categorical].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.88363Z","iopub.execute_input":"2021-12-16T13:37:43.88393Z","iopub.status.idle":"2021-12-16T13:37:43.920371Z","shell.execute_reply.started":"2021-12-16T13:37:43.883852Z","shell.execute_reply":"2021-12-16T13:37:43.919898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple feature:: by converting all our categorical vals to numerical vals","metadata":{}},{"cell_type":"code","source":"##lets import all relevant libraries\nfrom sklearn.feature_extraction import DictVectorizer\n\n## converting our dataframe into a list of dictionaries\ntrain_dict = df_train[categorical + numerical].to_dict(orient='records')\n\n## lets instantiate the \ndv = DictVectorizer(sparse=False)\n\n##\ndv.fit(train_dict)\n\n##\nX_train = dv.transform(train_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:43.921259Z","iopub.execute_input":"2021-12-16T13:37:43.921414Z","iopub.status.idle":"2021-12-16T13:37:44.096045Z","shell.execute_reply.started":"2021-12-16T13:37:43.921383Z","shell.execute_reply":"2021-12-16T13:37:44.095544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train our base model","metadata":{}},{"cell_type":"code","source":"##import all relevant libraries\nfrom sklearn.linear_model import LogisticRegression\n\n## create an instante\nmodel = LogisticRegression(solver='liblinear', random_state=1)\n\n##train our model\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.097049Z","iopub.execute_input":"2021-12-16T13:37:44.097323Z","iopub.status.idle":"2021-12-16T13:37:44.226153Z","shell.execute_reply.started":"2021-12-16T13:37:44.097299Z","shell.execute_reply":"2021-12-16T13:37:44.225372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate our models performance ","metadata":{}},{"cell_type":"code","source":"##\nval_dict = df_val[categorical + numerical].to_dict(orient='records')\n\n#dv.fit(val_di\n\nX_val = dv.transform(val_dict)\n\ny_pred = model.predict_proba(X_val)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.227149Z","iopub.execute_input":"2021-12-16T13:37:44.227304Z","iopub.status.idle":"2021-12-16T13:37:44.297993Z","shell.execute_reply.started":"2021-12-16T13:37:44.227284Z","shell.execute_reply":"2021-12-16T13:37:44.297334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Small subset","metadata":{}},{"cell_type":"code","source":"small_subset = ['contract', 'tenure', 'totalcharges']\n\ntrain_dict_small_subset = df_train[small_subset].to_dict(orient='records')\n\ndv = DictVectorizer(sparse=False)\n\ndv.fit(train_dict_small_subset)\n\nX_small_train = dv.transform(train_dict_small_subset)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.29905Z","iopub.execute_input":"2021-12-16T13:37:44.299614Z","iopub.status.idle":"2021-12-16T13:37:44.337413Z","shell.execute_reply.started":"2021-12-16T13:37:44.299584Z","shell.execute_reply":"2021-12-16T13:37:44.336838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets train\nfrom sklearn.linear_model import LogisticRegression\n\nmodel_small = LogisticRegression(solver='liblinear', random_state=1)\n\nmodel_small.fit(X_small_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.338515Z","iopub.execute_input":"2021-12-16T13:37:44.338866Z","iopub.status.idle":"2021-12-16T13:37:44.369977Z","shell.execute_reply.started":"2021-12-16T13:37:44.338837Z","shell.execute_reply":"2021-12-16T13:37:44.3694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets evaluate our model\nval_dict_small_subset = df_val[small_subset].to_dict(orient='records')\n\n#dv.fit(val_dict_small_subset)\n\nX_small_val = dv.transform(val_dict_small_subset)\n\ny_pred_small = model_small.predict_proba(X_small_val)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.371208Z","iopub.execute_input":"2021-12-16T13:37:44.371542Z","iopub.status.idle":"2021-12-16T13:37:44.401611Z","shell.execute_reply.started":"2021-12-16T13:37:44.371509Z","shell.execute_reply":"2021-12-16T13:37:44.400934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy","metadata":{}},{"cell_type":"code","source":"##get the predictions from our model\ny_pred = model.predict_proba(X_val)[:,1]\n\n## make hard predictions\nchurn = y_pred >= 0.5\n\n## computes the accuracy\n(y_val == churn).mean()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.402816Z","iopub.execute_input":"2021-12-16T13:37:44.403163Z","iopub.status.idle":"2021-12-16T13:37:44.410635Z","shell.execute_reply.started":"2021-12-16T13:37:44.40313Z","shell.execute_reply":"2021-12-16T13:37:44.409899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n##lets create an area with different thresholds\nthresholds = np.linspace(0,1,11)\n\n##lets loop over our threshold values\nfor t in thresholds:\n    ##lets make the hard predictions\n    churn = y_pred >= t\n    ##lets use accuracy_score from scikit learn\n    acc = accuracy_score(y_val, churn)\n    ##lets print the thresholds and the accuracy values to std ouput\n    print('%0.2f %0.3f' % (t, acc))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.411836Z","iopub.execute_input":"2021-12-16T13:37:44.412194Z","iopub.status.idle":"2021-12-16T13:37:44.428956Z","shell.execute_reply.started":"2021-12-16T13:37:44.41216Z","shell.execute_reply":"2021-12-16T13:37:44.428149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets visualize our accuracy score with respect to each threshold value","metadata":{}},{"cell_type":"code","source":"thresholds = np.linspace(0,1,21)\n\n##\naccuracies = []\n\n##\nfor t in thresholds:\n    acc = accuracy_score(y_val, y_pred >= t)\n    accuracies.append(acc)\n    \nplt.plot(thresholds, accuracies)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.430214Z","iopub.execute_input":"2021-12-16T13:37:44.430571Z","iopub.status.idle":"2021-12-16T13:37:44.617148Z","shell.execute_reply.started":"2021-12-16T13:37:44.430537Z","shell.execute_reply":"2021-12-16T13:37:44.615859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets also compute the accuracy on our validation set\nval_dict_small = df_val[small_subset].to_dict(orient='records')\n\nX_small_val = dv.transform(val_dict_small)\n\ny_pred_small = model_small.predict_proba(X_small_val)[:,1]\n\nchurn_small = y_pred_small >= 0.5\naccuracy_score(y_val, churn_small)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.618113Z","iopub.execute_input":"2021-12-16T13:37:44.618298Z","iopub.status.idle":"2021-12-16T13:37:44.645018Z","shell.execute_reply.started":"2021-12-16T13:37:44.618276Z","shell.execute_reply":"2021-12-16T13:37:44.644414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy baseline","metadata":{}},{"cell_type":"code","source":"size_val = len(y_val)\nbaseline = np.repeat(False, size_val)\nbaseline","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.646172Z","iopub.execute_input":"2021-12-16T13:37:44.646741Z","iopub.status.idle":"2021-12-16T13:37:44.65273Z","shell.execute_reply.started":"2021-12-16T13:37:44.646707Z","shell.execute_reply":"2021-12-16T13:37:44.652119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(baseline, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.654142Z","iopub.execute_input":"2021-12-16T13:37:44.654575Z","iopub.status.idle":"2021-12-16T13:37:44.669415Z","shell.execute_reply.started":"2021-12-16T13:37:44.654526Z","shell.execute_reply":"2021-12-16T13:37:44.668702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusioin table \nThis refers to a table that concisely represents every possible outcome of our models predictions.\nNamely; True Positve, True Negative, False Positive, and False Negative.","metadata":{}},{"cell_type":"code","source":"##lets set our predictions at the threshold value of 0.5 \nt = 0.5 \npredicted_churn = (y_pred >= t)\npredicted_no_churn = (y_pred < t)\n\n## lets get the actual targets \nactual_churn = (y_val == 1)\nactual_no_churn = (y_val == 0)\n\n## lets computes true positives \ntrue_positive = (predicted_churn & actual_churn).sum()\nfalse_positive = (predicted_churn & actual_no_churn).sum()\n\n##lets computes true negatives\ntrue_negative = (predicted_no_churn & actual_no_churn).sum()\nfalse_negative = (predicted_no_churn & actual_churn).sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.670804Z","iopub.execute_input":"2021-12-16T13:37:44.671227Z","iopub.status.idle":"2021-12-16T13:37:44.684409Z","shell.execute_reply.started":"2021-12-16T13:37:44.67119Z","shell.execute_reply":"2021-12-16T13:37:44.683756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets put all the values together\nconfusion_table = np.array(\n    [[true_negative, false_positive],\n    [false_negative, true_positive]]\n)\n\nconfusion_table","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.685952Z","iopub.execute_input":"2021-12-16T13:37:44.686366Z","iopub.status.idle":"2021-12-16T13:37:44.702294Z","shell.execute_reply.started":"2021-12-16T13:37:44.686329Z","shell.execute_reply":"2021-12-16T13:37:44.701716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_table / confusion_table.sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.703577Z","iopub.execute_input":"2021-12-16T13:37:44.703981Z","iopub.status.idle":"2021-12-16T13:37:44.71696Z","shell.execute_reply.started":"2021-12-16T13:37:44.703944Z","shell.execute_reply":"2021-12-16T13:37:44.716367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = true_positive / (true_positive + false_positive)\nrecall = true_positive / (true_positive + false_negative)\nprecision, recall ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.717819Z","iopub.execute_input":"2021-12-16T13:37:44.718567Z","iopub.status.idle":"2021-12-16T13:37:44.730583Z","shell.execute_reply.started":"2021-12-16T13:37:44.71853Z","shell.execute_reply":"2021-12-16T13:37:44.730042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating a model at multiple thresholds ","metadata":{}},{"cell_type":"code","source":"## lets create a list where we'll keep the results\nscores = []\n\n## lets create an array with different threshold values\n\n## lets loop through them...\nthresholds = np.linspace(0, 1, 101)\n\n## computes the confusion table for predictions at each threshold\nfor t in thresholds:\n    tp = ((y_pred >= t) & (y_val == 1)).sum()\n    fp = ((y_pred >= t) & (y_val == 0)).sum()\n    fn = ((y_pred < t) & (y_val == 1)).sum()\n    tn = ((y_pred < t) & (y_val == 0)).sum()\n    \n    ##lets append the resutls to the score list\n    scores.append((t, tp, fp, fn, tn))\n\n    \n## lets turn the list into a pandas dataframe \ndf_scores = pd.DataFrame(scores)\n\n##assigns names to the columns of the dataframe \ndf_scores.columns = ['threshold', 'tp', 'fp', 'fn', 'tn']","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.736589Z","iopub.execute_input":"2021-12-16T13:37:44.737284Z","iopub.status.idle":"2021-12-16T13:37:44.754297Z","shell.execute_reply.started":"2021-12-16T13:37:44.737245Z","shell.execute_reply":"2021-12-16T13:37:44.753677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scores[::10]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.755941Z","iopub.execute_input":"2021-12-16T13:37:44.756329Z","iopub.status.idle":"2021-12-16T13:37:44.768112Z","shell.execute_reply.started":"2021-12-16T13:37:44.756294Z","shell.execute_reply":"2021-12-16T13:37:44.767505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets compute the TPR and FPR scores\ndf_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\ndf_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.769195Z","iopub.execute_input":"2021-12-16T13:37:44.769419Z","iopub.status.idle":"2021-12-16T13:37:44.777849Z","shell.execute_reply.started":"2021-12-16T13:37:44.769388Z","shell.execute_reply":"2021-12-16T13:37:44.777266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scores[::10]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.779082Z","iopub.execute_input":"2021-12-16T13:37:44.779368Z","iopub.status.idle":"2021-12-16T13:37:44.800136Z","shell.execute_reply.started":"2021-12-16T13:37:44.779341Z","shell.execute_reply":"2021-12-16T13:37:44.799121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(df_scores.threshold, df_scores.tpr, linestyle='solid' , label='TPR')\nplt.plot(df_scores.threshold, df_scores.fpr, linestyle='dashed',  label='FPR')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.80128Z","iopub.execute_input":"2021-12-16T13:37:44.802014Z","iopub.status.idle":"2021-12-16T13:37:44.967749Z","shell.execute_reply.started":"2021-12-16T13:37:44.80192Z","shell.execute_reply":"2021-12-16T13:37:44.967327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random baseline model  \nThis model outputs a random score between 0 and 1, regardless of the input. ","metadata":{}},{"cell_type":"code","source":"##lets create a function to cal TRP and FPR at diff thresholds\ndef tpr_fpr_dataframe(y_val, y_pred):\n    \"\"\"\n    Defines a function that takes in actual and predicted values\n    \"\"\"\n    ##empty list of scores\n    scores = []\n    \n    ##create an numpy array of threholds value\n    thresholds = np.linspace(0, 1, 101)\n    \n    ##cal the confusion table for different thresholds\n    \n    for t in thresholds:\n        tp = ((y_pred >= t) & (y_val == 1)).sum()\n        fp = ((y_pred >= t) & (y_val == 0)).sum()\n        fn = ((y_pred < t) & (y_val == 1)).sum()\n        tn = ((y_pred < t) & (y_val == 0)).sum()\n        scores.append((t, tp, fp, fn, tn))\n     \n    ##lets converts the confusion table numbers to a dataframe\n    df_scores = pd.DataFrame(scores)\n    df_scores.columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n    \n    ##cal TPR and FPR using the confusion table numbers\n    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\n    \n    return df_scores","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.96879Z","iopub.execute_input":"2021-12-16T13:37:44.969768Z","iopub.status.idle":"2021-12-16T13:37:44.978455Z","shell.execute_reply.started":"2021-12-16T13:37:44.969733Z","shell.execute_reply":"2021-12-16T13:37:44.977487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets fix the random seet \nnp.random.seed(1)\n\n##generate an array with random numbers b/n 0 and 1\ny_rand = np.random.uniform(0, 1, size=len(y_val))\n\n##lets use this function to cal the TPR and FPR\ndf_rand = tpr_fpr_dataframe(y_val, y_rand)\n\n##display \ndf_rand[::10]\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:44.979443Z","iopub.execute_input":"2021-12-16T13:37:44.979606Z","iopub.status.idle":"2021-12-16T13:37:45.009786Z","shell.execute_reply.started":"2021-12-16T13:37:44.979585Z","shell.execute_reply":"2021-12-16T13:37:45.008687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\n\nplt.plot(df_rand.threshold, df_rand.tpr, label='TPR')\nplt.plot(df_rand.threshold, df_rand.fpr, label='FPR')\nplt.legend()\n\nplt.xticks(np.linspace(0,1,11))\nplt.yticks(np.linspace(0,1,11))\n\nplt.xlabel('Thresholds')\nplt.title('TPR and FPR for the random model')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.01172Z","iopub.execute_input":"2021-12-16T13:37:45.012292Z","iopub.status.idle":"2021-12-16T13:37:45.163254Z","shell.execute_reply.started":"2021-12-16T13:37:45.012259Z","shell.execute_reply":"2021-12-16T13:37:45.162447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Ideal model \nThe Ideal model always make a correct prediction\n","metadata":{}},{"cell_type":"code","source":"##lets calculates the number of neg and posi examples in the dataset. \nnum_neg = (y_val == 0).sum()\nnum_pos = (y_val == 1).sum()\n\n##lets generates an array that first repeats 0s num_neg num of times, \n##followed by 1s repeated num_pos number of times \ny_ideal = np.repeat([0,1], [num_neg, num_pos])\ny_pred_ideal = np.linspace(0, 1, num_neg + num_pos)\n\n##computes the TPR and FPR curves for classifier\ndf_ideal = tpr_fpr_dataframe(y_ideal, y_pred_ideal)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.164374Z","iopub.execute_input":"2021-12-16T13:37:45.164568Z","iopub.status.idle":"2021-12-16T13:37:45.179367Z","shell.execute_reply.started":"2021-12-16T13:37:45.164545Z","shell.execute_reply":"2021-12-16T13:37:45.178477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\n\nplt.plot(df_ideal.threshold, df_ideal.tpr, label='TPR')\nplt.plot(df_ideal.threshold, df_ideal.fpr, label='FPR')\nplt.legend()\n\nplt.xticks(np.linspace(0,1,11))\nplt.yticks(np.linspace(0, 1, 11))\n\nplt.vlines(1 - y_val.mean(), -1, 2, linewidth=0.5, linestyle='dashed', color='grey')\nplt.ylim(-0.03, 1.03)\n\nplt.xlabel('Thresholds')\nplt.title('TPR and FPR for the ideal model')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.180798Z","iopub.execute_input":"2021-12-16T13:37:45.1811Z","iopub.status.idle":"2021-12-16T13:37:45.324654Z","shell.execute_reply.started":"2021-12-16T13:37:45.180965Z","shell.execute_reply":"2021-12-16T13:37:45.32379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ROC curve","metadata":{}},{"cell_type":"code","source":"##lets make the plot square\nplt.figure(figsize=(5,5))\n\nplt.plot(df_scores.fpr, df_scores.tpr, color='black', label='Model')\nplt.plot(df_rand.fpr, df_rand.tpr, color='black', lw=1, linestyle='dashed', alpha=0.5, label='Random')\nplt.plot(df_ideal.fpr, df_ideal.tpr, color='black', lw=0.5, linestyle='solid', alpha=0.5, label='Ideal')\nplt.legend()\n\nplt.xlim([-0.02, 1.02])\nplt.ylim([-0.02, 1.02])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.title('ROC curve')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.325996Z","iopub.execute_input":"2021-12-16T13:37:45.326161Z","iopub.status.idle":"2021-12-16T13:37:45.509952Z","shell.execute_reply.started":"2021-12-16T13:37:45.32614Z","shell.execute_reply":"2021-12-16T13:37:45.509266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\n\nplt.plot(df_scores.fpr, df_scores.tpr, color='black')\nplt.plot([0, 1], [0, 1], color='black', lw=0.7, linestyle='dashed', alpha=0.5)\n\nplt.xlim([-0.02, 1.02])\nplt.ylim([-0.02, 1.02])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.title('ROC curve')\n\n# plt.savefig('04_roc_curve.svg')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.511051Z","iopub.execute_input":"2021-12-16T13:37:45.511222Z","iopub.status.idle":"2021-12-16T13:37:45.654155Z","shell.execute_reply.started":"2021-12-16T13:37:45.511199Z","shell.execute_reply":"2021-12-16T13:37:45.652951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting ROC Curve using Scikit learn","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\n\nplt.figure(figsize=(5,5))\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.655475Z","iopub.execute_input":"2021-12-16T13:37:45.655665Z","iopub.status.idle":"2021-12-16T13:37:45.80813Z","shell.execute_reply.started":"2021-12-16T13:37:45.655643Z","shell.execute_reply":"2021-12-16T13:37:45.80741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Lets plot the ROC curve of small and large models \nfpr_large, tpr_large, _ = roc_curve(y_val, y_pred)\nfpr_small, tpr_small, _ = roc_curve(y_val, y_pred_small)\n\nplt.figure(figsize=(5,5))\n\nplt.plot(fpr_large, tpr_large, label='Large')\nplt.plot(fpr_small, tpr_small, label='Small')\nplt.plot([0,1], [0,1])\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:45.809902Z","iopub.execute_input":"2021-12-16T13:37:45.810136Z","iopub.status.idle":"2021-12-16T13:37:46.189673Z","shell.execute_reply.started":"2021-12-16T13:37:45.810106Z","shell.execute_reply":"2021-12-16T13:37:46.188789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Area under the ROC (AUC) ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import auc\n\nauc(df_scores.fpr, df_scores.tpr)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:46.191016Z","iopub.execute_input":"2021-12-16T13:37:46.191237Z","iopub.status.idle":"2021-12-16T13:37:46.199025Z","shell.execute_reply.started":"2021-12-16T13:37:46.191209Z","shell.execute_reply":"2021-12-16T13:37:46.198083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets cal auc for the small model\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:46.200236Z","iopub.execute_input":"2021-12-16T13:37:46.200431Z","iopub.status.idle":"2021-12-16T13:37:46.213124Z","shell.execute_reply.started":"2021-12-16T13:37:46.200408Z","shell.execute_reply":"2021-12-16T13:37:46.212171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_val, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:46.21497Z","iopub.execute_input":"2021-12-16T13:37:46.215179Z","iopub.status.idle":"2021-12-16T13:37:46.229933Z","shell.execute_reply.started":"2021-12-16T13:37:46.215157Z","shell.execute_reply":"2021-12-16T13:37:46.228677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's put all the code for training into a train function, which first converts the data into a one-hot encoding representation and then trains the model.","metadata":{}},{"cell_type":"code","source":"def train(df, y):\n    ##applies one-hot encoding\n    cat = df[categorical + numerical].to_dict(orient='records')\n    \n    dv = DictVectorizer(sparse=False)\n    dv.fit(cat)\n    \n    X = dv.transform(cat)\n    ##trains the model\n    model = LogisticRegression(solver='liblinear')\n    \n    model.fit(X, y)\n    \n    return dv, model","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:46.231491Z","iopub.execute_input":"2021-12-16T13:37:46.231679Z","iopub.status.idle":"2021-12-16T13:37:46.239593Z","shell.execute_reply.started":"2021-12-16T13:37:46.231656Z","shell.execute_reply":"2021-12-16T13:37:46.238544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Likewise, we also put the prediction logic into a predict function. \nThis function takes in a dataframe with customers,\nthe vectorizer we “trained” previously — for doing onehot encoding — and the model. \nThen we apply the vectorizer to the dataframe, get a matrix, \nand finally apply the model to the matrix to get predictions","metadata":{}},{"cell_type":"code","source":"def predict(df, dv, model):\n    cat = df[categorical + numerical].to_dict(orient='records')\n    \n    X = dv.transform(cat)\n    y_pred = model.predict_proba(X)[:,1]\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:46.24107Z","iopub.execute_input":"2021-12-16T13:37:46.241313Z","iopub.status.idle":"2021-12-16T13:37:46.251856Z","shell.execute_reply.started":"2021-12-16T13:37:46.241283Z","shell.execute_reply":"2021-12-16T13:37:46.251404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use these functions for implementing K-fold cross-validation.\n","metadata":{}},{"cell_type":"code","source":"##lets import the KFold class\nfrom sklearn.model_selection import KFold\n\n## lets use it to split the data into 10 parts\nkfold = KFold(n_splits=10, shuffle=True, random_state=1)\n\n## creates a list for storing the resutls \naucs = []\n\n##lets iterate over the 10 diff splits of the data\nfor train_idx, val_idx in kfold.split(df_train_full):\n    ##lets splits the data into train and validation sets\n    df_train = df_train_full.iloc[train_idx]\n    df_val = df_train_full.iloc[val_idx]\n    \n    y_train = df_train.churn.values\n    y_val = df_val.churn.values \n    \n    #trains the model and makes predictions\n    dv, model = train(df_train, y_train)\n    y_pred = predict(df_val, dv, model)\n    \n    ## lets evaluate the quality of the train on the validation data using AUC\n    auc = roc_auc_score(y_val, y_pred)\n    aucs.append(auc)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:46.253224Z","iopub.execute_input":"2021-12-16T13:37:46.253431Z","iopub.status.idle":"2021-12-16T13:37:49.270401Z","shell.execute_reply.started":"2021-12-16T13:37:46.253402Z","shell.execute_reply":"2021-12-16T13:37:49.269863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('auc = %0.3f ± %0.3f' % (np.mean(aucs), np.std(aucs)))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:49.271631Z","iopub.execute_input":"2021-12-16T13:37:49.272002Z","iopub.status.idle":"2021-12-16T13:37:49.278542Z","shell.execute_reply.started":"2021-12-16T13:37:49.271971Z","shell.execute_reply":"2021-12-16T13:37:49.277608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding the best parameters\nLet’s select our cross-validation procedure for selecting the best parameter C. For\nthat, we first adjust the train function to take in an additional parameter","metadata":{}},{"cell_type":"code","source":"def train(df, y, C):\n    cat = df[categorical + numerical].to_dict(orient='records')\n    \n    dv = DictVectorizer(sparse=False)\n    \n    dv.fit(cat)\n    \n    X = dv.transform(cat)\n    \n    ##lets use the parameter during training\n    model = LogisticRegression(solver='liblinear', C=C)\n    model.fit(X, y)\n    \n    return dv, model","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:49.281909Z","iopub.execute_input":"2021-12-16T13:37:49.282144Z","iopub.status.idle":"2021-12-16T13:37:49.293943Z","shell.execute_reply.started":"2021-12-16T13:37:49.282117Z","shell.execute_reply":"2021-12-16T13:37:49.293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" let’s find the best parameter C. The idea is simple:\n Loop over different values of C.\nFor each C, run cross-validation and record the mean AUC across all folds as\nwell as the standard deviation","metadata":{}},{"cell_type":"code","source":"##Tuning the model: selecting the best parameter C usinng cross-validation\nnfolds = 5\nkfolds = KFold(n_splits=nfolds, shuffle=True, random_state=1)\n\nfor C in [0.001, 0.01, 0.1, 0.5, 1, 10]:\n    aucs = []\n    \n    for train_idx, val_idx in kfold.split(df_train_full):\n        df_train = df_train_full.iloc[train_idx]\n        df_val = df_train_full.iloc[val_idx]\n        \n        y_train = df_train.churn.values\n        y_val = df_val.churn.values\n        \n        dv, model = train(df_train, y_train, C=C)\n        y_pred = predict(df_val, dv, model)\n        \n        auc = roc_auc_score(y_val, y_pred)\n        aucs.append(auc)\n        \n    print('C=%s, auc = %0.3f += %0.3f' % (C, np.mean(aucs), np.std(aucs)))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:37:49.295176Z","iopub.execute_input":"2021-12-16T13:37:49.295366Z","iopub.status.idle":"2021-12-16T13:38:07.254891Z","shell.execute_reply.started":"2021-12-16T13:37:49.29534Z","shell.execute_reply":"2021-12-16T13:38:07.253972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train the model on the entire train and validation\ndatasets and apply it to the test dataset to verify it indeed works well.\nLet’s use our train and predict functions for that:","metadata":{"execution":{"iopub.status.busy":"2021-12-16T10:47:37.304258Z","iopub.execute_input":"2021-12-16T10:47:37.304581Z","iopub.status.idle":"2021-12-16T10:47:37.310392Z","shell.execute_reply.started":"2021-12-16T10:47:37.304547Z","shell.execute_reply":"2021-12-16T10:47:37.309572Z"}}},{"cell_type":"code","source":"y_train = df_train_full.churn.values\ny_test = df_test.churn.values\n\n##lets train the modelon the full training dataset\ndv, model = train(df_train_full, y_train, C=0.5)\n##applies it to the test dataset\ny_pred = predict(df_test, dv, model)\n\n## evaluates the predictions on the test data\nauc = roc_auc_score(y_test, y_pred)\nprint('auc = %.3f' % auc)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:38:07.25612Z","iopub.execute_input":"2021-12-16T13:38:07.256314Z","iopub.status.idle":"2021-12-16T13:38:07.613518Z","shell.execute_reply.started":"2021-12-16T13:38:07.256287Z","shell.execute_reply":"2021-12-16T13:38:07.613024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer = {\n'customerid': '8879-zkjof',\n'gender': 'female',\n'seniorcitizen': 0,\n'partner': 'no',\n'dependents': 'no',\n'tenure': 41,\n'phoneservice': 'yes',\n'multiplelines': 'no',\n'internetservice': 'dsl',\n'onlinesecurity': 'yes',\n'onlinebackup': 'no',\n'deviceprotection': 'yes',\n'techsupport': 'yes',\n'streamingtv': 'yes',\n'streamingmovies': 'yes',\n'contract': 'one_year',\n'paperlessbilling': 'yes',\n'paymentmethod': 'bank_transfer_(automatic)',\n'monthlycharges': 79.85,\n'totalcharges': 3320.75,\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:38:26.934766Z","iopub.execute_input":"2021-12-16T13:38:26.935092Z","iopub.status.idle":"2021-12-16T13:38:26.942005Z","shell.execute_reply.started":"2021-12-16T13:38:26.935062Z","shell.execute_reply":"2021-12-16T13:38:26.940723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([customer])\ny_pred = predict(df, dv, model)\ny_pred[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:39:16.132362Z","iopub.execute_input":"2021-12-16T13:39:16.132573Z","iopub.status.idle":"2021-12-16T13:39:16.144159Z","shell.execute_reply.started":"2021-12-16T13:39:16.132552Z","shell.execute_reply":"2021-12-16T13:39:16.142797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(df, dv, model):\n    cat = df[categorical + numerical].to_dict(orient='rows')\n    X = dv.transform(cat)\n    y_pred = model.predict_proba(X)[:, 1]\n    return y_pred ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:42:19.254036Z","iopub.execute_input":"2021-12-16T13:42:19.254294Z","iopub.status.idle":"2021-12-16T13:42:19.259528Z","shell.execute_reply.started":"2021-12-16T13:42:19.254267Z","shell.execute_reply":"2021-12-16T13:42:19.258507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets create a function to predict the prob of churn for a single customer only\ndef predict_single(customer, dv, model):\n    ## vectorizes the customer: creates the matri\n    X = dv.transform([customer])\n    ## applies the model to this matrix \n    y_pred = model.predict_proba(X)[:,1]\n    ## lets return the first element of the result\n    return y_pred[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:50:39.301205Z","iopub.execute_input":"2021-12-16T13:50:39.301554Z","iopub.status.idle":"2021-12-16T13:50:39.308156Z","shell.execute_reply.started":"2021-12-16T13:50:39.301518Z","shell.execute_reply":"2021-12-16T13:50:39.307089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##\npredict_single(customer, dv, model)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:50:59.062534Z","iopub.execute_input":"2021-12-16T13:50:59.063036Z","iopub.status.idle":"2021-12-16T13:50:59.068788Z","shell.execute_reply.started":"2021-12-16T13:50:59.063007Z","shell.execute_reply":"2021-12-16T13:50:59.068022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}